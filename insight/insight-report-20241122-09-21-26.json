{"amount_correct": 6, "percentage_score": 30, "report_time": "2024-11-22 14:21:26", "checks": [{"description": "Ensure that the README.md file exists inside of the root of the GitHub repository", "check": "ConfirmFileExists", "status": true, "path": "../README.md"}, {"description": "Delete the phrase 'Add Your Name Here' and add your own name as an Honor Code pledge in README.md", "check": "MatchFileFragment", "options": {"fragment": "Add Your Name Here", "count": 0, "exact": true}, "status": false, "path": "../README.md", "diagnostic": "Found 2 fragment(s) in the README.md or the output while expecting exactly 0"}, {"description": "Complete all TODOs, remove the TODO markers, and rewrite comments for README.md", "check": "MatchFileFragment", "options": {"fragment": "TODO", "count": 0, "exact": true}, "status": false, "path": "../README.md", "diagnostic": "Found 5 fragment(s) in the README.md or the output while expecting exactly 0"}, {"description": "Retype the every word in the Honor Code pledge in README.md", "check": "MatchFileFragment", "options": {"fragment": "I adhered to the Allegheny College Honor Code while completing this programming skill check.", "count": 3, "exact": true}, "status": true, "path": "../README.md"}, {"description": "Indicate that you have completed all of the tasks in the README.md", "check": "MatchFileFragment", "options": {"fragment": "- [X]", "count": 10, "exact": true}, "status": false, "path": "../README.md", "diagnostic": "Found 0 fragment(s) in the README.md or the output while expecting exactly 10"}, {"description": "Ensure that question_one.py file exists in the questions/ directory", "check": "ConfirmFileExists", "status": true, "path": "questions/question_one.py"}, {"description": "Complete all TODOs, remove the TODO markers, and rewrite comments for question_one.py", "check": "MatchFileFragment", "options": {"fragment": "TODO", "count": 0, "exact": true}, "status": false, "path": "questions/question_one.py", "diagnostic": "Found 11 fragment(s) in the question_one.py or the output while expecting exactly 0"}, {"description": "Create a sufficient number of docstring (i.e., multiple-line) comments in question_one.py", "check": "CountMultipleLineComments", "options": {"language": "Python", "count": 10, "exact": false}, "status": false, "path": "questions/question_one.py", "diagnostic": "Found 9 comment(s) in the question_one.py or the output"}, {"description": "Create a sufficient number of single-line comments in question_one.py", "check": "CountSingleLineComments", "options": {"language": "Python", "count": 20, "exact": false}, "status": true, "path": "questions/question_one.py"}, {"description": "Ensure that test_question_one.py file exists in the tests/ directory", "check": "ConfirmFileExists", "status": true, "path": "tests/test_question_one.py"}, {"description": "Run checks for Question 1 Part (a) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_a\" --no-fancy --report status --report trace --report failure --report setup --report code", "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - tldr: None\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_one_part_a\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f6eba7b3800>\n     \n     Test Trace\n     \n     FAILED tests/test_question_one.py::test_compute_coverage_difference - AssertionError: Failed on case with identical \n     coverage reports\n     \n     test_question_one.py::test_compute_coverage_difference\n       - Status: Failed\n         Line: 31\n         Exact: [] == [CoverageItem...covered=True)] ...\n         Message: Failed on case with identical coverage reports\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_one.py::test_compute_coverage_difference\n       Path: <...>/programming-skill-check-four-starter/exam/tests/test_question_one.py\n       Line number: 31\n       Message: AssertionError: Failed on case with identical coverage reports\n     assert [] == [CoverageItem...covered=True)]\n       \n       Right contains 3 more items, first extra item: CoverageItem(id=1, line='line1', covered=True)\n       \n       Full diff:\n       + []\n       - [\n       -     CoverageItem(id=1, line='line1', covered=True),\n       -     CoverageItem(id=2, line='line2', covered=True),\n       -     CoverageItem(id=3, line='line3', covered=True),\n       - ]\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 19\n     @pytest.mark.question_one_part_a\n     def test_compute_coverage_difference():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         item1 = CoverageItem(1, \"line1\", True)\n         item2 = CoverageItem(2, \"line2\", True)\n         item3 = CoverageItem(3, \"line3\", True)\n         item4 = CoverageItem(4, \"line4\", True)\n         item5 = CoverageItem(5, \"line5\", True)\n         item6 = CoverageItem(6, \"line6\", True)\n         item7 = CoverageItem(1, \"line1\", False)\n         item8 = CoverageItem(2, \"line2\", False)\n         item9 = CoverageItem(3, \"line3\", False)\n         assert compute_coverage_intersection(\n             [item1, item2, item3], [item1, item2, item3]\n         ) == [\n             item1,\n             item2,\n             item3,\n         ], \"Failed on case with identical coverage reports\"\n         assert (\n             compute_coverage_intersection([item1, item2, item3], [item4, item5, item6])\n             == []\n         ), \"Failed on case with no common coverage\"\n         assert compute_coverage_intersection(\n             [item1, item2, item3], [item2, item3, item4]\n         ) == [\n             item2,\n             item3,\n         ], \"Failed on case with partial overlap\"\n         assert compute_coverage_intersection(\n             [item1, item2, item3], [item3, item2, item1]\n         ) == [\n             item1,\n             item2,\n             item3,\n         ], \"Failed on case with identical coverage reports in different order\"\n         assert (\n             compute_coverage_intersection([], []) == []\n         ), \"Failed on case with empty coverage reports\"\n         assert (\n             compute_coverage_intersection([item1, item2, item3], [item7, item8, item9])\n             == []\n         ), \"Failed on case with same ids but not covered\"\n         assert (\n             compute_coverage_difference([item1, item2, item3], [item1, item2, item3]) == []\n         ), \"Failed on case with identical coverage reports\"\n         assert compute_coverage_difference(\n             [item1, item2, item3], [item4, item5, item6]\n         ) == [\n             item1,\n             item2,\n             item3,\n         ], \"Failed on case with no common coverage\"\n         assert compute_coverage_difference(\n             [item1, item2, item3], [item2, item3, item4]\n         ) == [item1], \"Failed on case with partial overlap\"\n         assert (\n             compute_coverage_difference([item1, item2, item3], [item3, item2, item1]) == []\n         ), \"Failed on case with identical coverage reports in different order\"\n         assert (\n             compute_coverage_difference([], []) == []\n         ), \"Failed on case with empty coverage reports\"\n         assert compute_coverage_difference(\n             [item1, item2, item3], [item7, item8, item9]\n         ) == [\n             item1,\n             item2,\n             item3,\n         ], \"Failed on case with same ids but different coverage status\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 1 Part (b) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_b\" --no-fancy --report status --report trace --report failure --report setup --report code", "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - tldr: None\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_one_part_b\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f26fa5af920>\n     \n     Test Trace\n     \n     FAILED tests/test_question_one.py::test_compute_coverage_score - AssertionError: Failed on all items covered\n     \n     test_question_one.py::test_compute_coverage_score\n       - Status: Passed\n         Line: 94\n         Code: calculate_coverage_score([]) == 0.0\n         Exact: 0.0 == 0.0 ...\n       - Status: Failed\n         Line: 97\n         Exact: 0.0 == 1.0 ...\n         Message: Failed on all items covered\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_one.py::test_compute_coverage_score\n       Path: <...>/programming-skill-check-four-starter/exam/tests/test_question_one.py\n       Line number: 97\n       Message: AssertionError: Failed on all items covered\n     assert 0.0 == 1.0\n      +  where 0.0 = calculate_coverage_score([CoverageItem(id=1, line='line1', covered=True), CoverageItem(id=1, \n     line='line1', covered=True), CoverageItem(id=1, line='line1', covered=True), CoverageItem(id=1, line='line1', \n     covered=True), CoverageItem(id=1, line='line1', covered=True)])\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 90\n     @pytest.mark.question_one_part_b\n     def test_compute_coverage_score():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         # test with an empty list\n         assert calculate_coverage_score([]) == 0.0, \"Failed on empty list\"\n         # test with all items covered\n         all_covered = [CoverageItem(1, \"line1\", True) for _ in range(5)]\n         assert calculate_coverage_score(all_covered) == 1.0, \"Failed on all items covered\"\n         # test with no items covered\n         none_covered = [CoverageItem(1, \"line1\", False) for _ in range(5)]\n         assert calculate_coverage_score(none_covered) == 0.0, \"Failed on no items covered\"\n         # test with some items covered\n         some_covered = [\n             CoverageItem(1, \"line1\", True),\n             CoverageItem(2, \"line2\", False),\n             CoverageItem(3, \"line3\", True),\n         ]\n         assert (\n             calculate_coverage_score(some_covered) == 2 / 3\n         ), \"Failed on some items covered\"\n         # test with one item covered\n         one_covered = [CoverageItem(1, \"line1\", True)]\n         assert calculate_coverage_score(one_covered) == 1.0, \"Failed on one item covered\"\n         # test with one item not covered\n         one_not_covered = [CoverageItem(1, \"line1\", False)]\n         assert (\n             calculate_coverage_score(one_not_covered) == 0.0\n         ), \"Failed on one item not covered\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 1 Part (c) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_c\" --no-fancy --report status --report trace --report failure --report setup --report code", "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - tldr: None\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_one_part_c\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f5e70886c60>\n     \n     Test Trace\n     \n     FAILED tests/test_question_one.py::test_read_coverage_report_from_json - AssertionError: Expected 2 coverage items, got \n     0\n     \n     test_question_one.py::test_read_coverage_report_from_json\n       - Status: Failed\n         Line: 135\n         Exact: 0 == 2 ...\n         Message: Expected 2 coverage items, got 0\n     \n     test_question_one.py::test_read_coverage_report_from_empty_json\n       - Status: Passed\n         Line: 178\n         Code: len(coverage_report) == 0\n         Exact: 0 == 0 ...\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_one.py::test_read_coverage_report_from_json\n       Path: <...>/programming-skill-check-four-starter/exam/tests/test_question_one.py\n       Line number: 135\n       Message: AssertionError: Expected 2 coverage items, got 0\n     assert 0 == 2\n      +  where 0 = len([])\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 120\n     @pytest.mark.question_one_part_c\n     def test_read_coverage_report_from_json(tmp_path):\n         \"\"\"Confirm correctness of question part.\"\"\"\n         # define the JSON content for the test\n         coverage_data = [\n             {\"id\": 1, \"line\": \"int a = 0\", \"covered\": True},\n             {\"id\": 2, \"line\": \"    pass\", \"covered\": False},\n         ]\n         # create a temporary file and write the JSON content to it\n         temp_file = tmp_path / \"coverage_report.json\"\n         with temp_file.open(\"w\") as f:\n             json.dump(coverage_data, f)\n         # read the coverage report from the temporary file\n         coverage_report = read_coverage_report_from_json(str(temp_file))\n         # assertions with diagnostic messages\n         assert len(coverage_report) == 2, \"Expected 2 coverage items, got {0}\".format(\n             len(coverage_report)\n         )\n         assert (\n             coverage_report[0].id == 1\n         ), \"Expected id 1 for the first item, got {0}\".format(coverage_report[0].id)\n         assert (\n             coverage_report[0].line == \"int a = 0\"\n         ), \"Expected line 'int a = 0' for the first item, got {0}\".format(\n             coverage_report[0].line\n         )\n         assert (\n             coverage_report[0].covered is True\n         ), \"Expected covered True for the first item, got {0}\".format(\n             coverage_report[0].covered\n         )\n         assert (\n             coverage_report[1].id == 2\n         ), \"Expected id 2 for the second item, got {0}\".format(coverage_report[1].id)\n         assert (\n             coverage_report[1].line == \"    pass\"\n         ), \"Expected line '    pass' for the second item, got {0}\".format(\n             coverage_report[1].line\n         )\n         assert (\n             coverage_report[1].covered is False\n         ), \"Expected covered False for the second item, got {0}\".format(\n             coverage_report[1].covered\n         )\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 1 Part (d) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_d\" --no-fancy --report status --report trace --report failure --report setup --report code", "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - tldr: None\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_one_part_d\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7fed7f0a3a10>\n     \n     Test Trace\n     \n     FAILED tests/test_question_one.py::test_read_coverage_report_from_yaml - AssertionError: Expected 2 coverage items, got \n     0\n     \n     test_question_one.py::test_read_coverage_report_from_yaml\n       - Status: Failed\n         Line: 198\n         Exact: 0 == 2 ...\n         Message: Expected 2 coverage items, got 0\n     \n     test_question_one.py::test_read_coverage_report_from_empty_yaml\n       - Status: Passed\n         Line: 239\n         Code: len(coverage_report) == 0\n         Exact: 0 == 0 ...\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_one.py::test_read_coverage_report_from_yaml\n       Path: <...>/programming-skill-check-four-starter/exam/tests/test_question_one.py\n       Line number: 198\n       Message: AssertionError: Expected 2 coverage items, got 0\n     assert 0 == 2\n      +  where 0 = len([])\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 183\n     @pytest.mark.question_one_part_d\n     def test_read_coverage_report_from_yaml(tmp_path):\n         \"\"\"Confirm correctness of question part.\"\"\"\n         # define the YAML content for the test\n         coverage_data = [\n             {\"id\": 1, \"line\": \"int a = 0\", \"covered\": True},\n             {\"id\": 2, \"line\": \"    pass\", \"covered\": False},\n         ]\n         # create a temporary file and write the YAML content to it\n         temp_file = tmp_path / \"coverage_report.yaml\"\n         with temp_file.open(\"w\") as f:\n             yaml.dump(coverage_data, f)\n         # read the coverage report from the temporary file\n         coverage_report = read_coverage_report_from_yaml(str(temp_file))\n         # assertions with diagnostic messages\n         assert len(coverage_report) == 2, \"Expected 2 coverage items, got {0}\".format(\n             len(coverage_report)\n         )\n         assert (\n             coverage_report[0].id == 1\n         ), \"Expected id 1 for the first item, got {0}\".format(coverage_report[0].id)\n         assert (\n             coverage_report[0].line == \"int a = 0\"\n         ), \"Expected line 'int a = 0' for the first item, got {0}\".format(\n             coverage_report[0].line\n         )\n         assert (\n             coverage_report[0].covered is True\n         ), \"Expected covered True for the first item, got {0}\".format(\n             coverage_report[0].covered\n         )\n         assert (\n             coverage_report[1].id == 2\n         ), \"Expected id 2 for the second item, got {0}\".format(coverage_report[1].id)\n         assert (\n             coverage_report[1].line == \"    pass\"\n         ), \"Expected line '    pass' for the second item, got {0}\".format(\n             coverage_report[1].line\n         )\n         assert (\n             coverage_report[1].covered is False\n         ), \"Expected covered False for the second item, got {0}\".format(\n             coverage_report[1].covered\n         )\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Ensure that Question 1 follows industry-standard rules using the command 'ruff check'", "command": "poetry run ruff check questions/question_one.py", "status": false, "diagnostic": "questions/question_one.py:8:1: I001 [*] Import block is un-sorted or un-formatted\n        |\n      6 |   # to the industry best practices for Python source code.\n      7 |   \n      8 | / import json\n      9 | | import yaml\n     10 | | from typing import List\n     11 | | \n     12 | | \n     13 | | # Introduction: Read This First! {{{\n        | |_^ I001\n     14 |   \n     15 |   # Keep in mind these considerations as you implement the required functions:\n        |\n        = help: Organize imports\n     \n     questions/question_one.py:8:8: F401 [*] `json` imported but unused\n        |\n      6 | # to the industry best practices for Python source code.\n      7 | \n      8 | import json\n        |        ^^^^ F401\n      9 | import yaml\n     10 | from typing import List\n        |\n        = help: Remove unused import: `json`\n     \n     questions/question_one.py:9:8: F401 [*] `yaml` imported but unused\n        |\n      8 | import json\n      9 | import yaml\n        |        ^^^^ F401\n     10 | from typing import List\n        |\n        = help: Remove unused import: `yaml`\n     \n     questions/question_one.py:193:5: D103 Missing docstring in public function\n         |\n     193 | def read_coverage_report_from_json(file_path: str) -> List[CoverageItem]:\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     194 |     coverage_report = []\n     195 |     return coverage_report\n         |\n     \n     Found 4 errors.\n     [*] 3 fixable with the `--fix` option."}, {"description": "Ensure that Question 1 adheres to an industry-standard format using the command 'ruff format'", "command": "poetry run ruff format questions/question_one.py --check", "status": true}, {"description": "Ensure that Question 1 has correct type annotations using the command 'mypy'", "command": "poetry run mypy questions/question_one.py", "status": false, "diagnostic": "questions/question_one.py:94: error: Need type annotation for \"coverage_intersection\" (hint: \"coverage_intersection: list[<type>] = ...\")  [var-annotated]\n     questions/question_one.py:104: error: Need type annotation for \"coverage_difference\" (hint: \"coverage_difference: list[<type>] = ...\")  [var-annotated]\n     questions/question_one.py:194: error: Need type annotation for \"coverage_report\" (hint: \"coverage_report: list[<type>] = ...\")  [var-annotated]\n     questions/question_one.py:269: error: Need type annotation for \"coverage_report\" (hint: \"coverage_report: list[<type>] = ...\")  [var-annotated]\n     Found 4 errors in 1 file (checked 1 source file)"}, {"description": "Ensure that Question 1 has correct number of fully type annotated functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --fully-typed -f questions/question_one.py --count", "fragment": 5, "count": 1, "exact": true}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 1 has correct number of documented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --documented -f questions/question_one.py --count", "fragment": 6, "count": 1, "exact": true}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 1 has no undocumented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --undocumented -f questions/question_one.py --count", "fragment": 0, "count": 1, "exact": true}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}]}